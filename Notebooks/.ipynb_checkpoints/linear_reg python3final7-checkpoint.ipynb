{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasu/PycharmProjects/EEG/test/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/vasu/PycharmProjects/EEG/test/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/vasu/PycharmProjects/EEG/test/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/vasu/PycharmProjects/EEG/test/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/vasu/PycharmProjects/EEG/test/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/vasu/PycharmProjects/EEG/test/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "rng = np.random\n",
    "from array import array\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 192, 1000)\n"
     ]
    }
   ],
   "source": [
    "# data import from matlab\n",
    "subject_1 = sio.loadmat('1.mat')      #recovering matlab data in the form of a python dictionar\n",
    "format_1 = subject_1['data']          #in the dictionary, only the data key interests us\n",
    "print (format_1.shape)\n",
    "\n",
    "# shuffle trials\n",
    "(channel, trial, time_points)= format_1.shape\n",
    "\n",
    "trials = np.arange(trial)\n",
    "np.random.shuffle(trials)\n",
    "\n",
    "#Z score\n",
    "format_1=stats.zscore(format_1, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98485181, -1.8921931 ,  0.29374336,  1.8889629 ,  0.43160567,\n",
       "        1.05819495,  1.37843573,  0.83705707,  0.93917554, -1.13875481,\n",
       "       -1.10181775, -1.05509029, -1.35368639,  0.21955415,  1.49036535,\n",
       "       -0.57089948,  0.28664177, -0.44451716, -0.48750655, -0.74375185,\n",
       "       -0.40828591, -0.18567557, -0.54867858, -1.44765027, -0.12459089,\n",
       "       -0.5590327 ,  1.0705511 , -1.44937522,  1.61902751,  1.47224841,\n",
       "        0.89571076,  0.18932623, -0.17825715,  0.13789774,  0.11982257,\n",
       "       -0.72821391,  0.11683494, -1.2980923 ,  0.97170626, -0.31476935,\n",
       "        1.08573063, -0.62431801,  0.31571583,  0.73579216, -1.59691686,\n",
       "        0.91016788, -0.23880956,  1.23944988,  1.69955122,  1.02634286,\n",
       "       -0.11856277,  0.96176269, -1.26107134, -1.17877759, -0.05456903,\n",
       "       -1.29548104, -0.04613751,  0.91589575,  0.0315261 ,  0.52774333,\n",
       "        0.26290713, -0.95813662, -1.05169198,  1.1549709 , -0.0894976 ,\n",
       "       -0.05776596,  1.25679018,  0.13539477, -1.07485555, -0.42037282,\n",
       "        1.26530206, -0.22088339, -1.17225382,  0.99746634, -0.65774185,\n",
       "        0.34696695, -0.29702423, -1.23736267, -0.50764366, -0.0942545 ,\n",
       "       -0.31697543, -1.8921931 ,  0.8310686 , -1.39088844,  0.40613745,\n",
       "       -0.00931667,  0.21557217,  0.84595623, -0.20097088,  1.05291693,\n",
       "        1.03530005,  1.10285365,  0.01206758, -1.85896717,  0.07251571,\n",
       "       -1.79924737, -0.09723018, -0.24502228,  0.32425475, -0.08665362,\n",
       "       -0.52694159, -0.33245242,  0.20968444, -0.12321013,  0.16531138,\n",
       "        1.04848379,  0.27723267, -0.97138759,  0.06040253,  0.42422895,\n",
       "       -1.13186789, -0.25060674,  1.01179304,  1.389659  , -1.26910093,\n",
       "        1.27390399, -0.80122668, -0.28172166,  1.63761487,  0.75833304,\n",
       "        0.46125963, -1.36333484, -1.34645719,  0.23511808,  0.16862437,\n",
       "       -1.2196292 ,  0.76116705,  1.28056772, -1.53075597, -0.0643867 ,\n",
       "        1.71900847, -1.33988894, -1.81481242,  1.28967813,  1.8889629 ,\n",
       "        0.10755824, -0.14305901, -0.88256265,  0.04234245, -1.8921931 ,\n",
       "       -1.03942003, -0.56294513, -1.15035126,  0.76662878,  0.48907003,\n",
       "        1.03748011, -0.82799926, -0.09986874, -1.8921931 ,  1.4133584 ,\n",
       "       -1.85886341, -0.72225522,  0.13473264, -0.89290804, -1.83542342,\n",
       "        0.09122733,  0.82297652,  0.98852222,  1.8889629 , -1.42659346,\n",
       "        1.25029552, -1.35604316,  1.75035503, -0.34302156, -0.32844554,\n",
       "       -1.47077391,  0.12968106, -0.90516046,  0.02124322,  0.97854568,\n",
       "        1.23682621,  1.05675726,  1.39435073,  1.8889629 , -0.29840725,\n",
       "       -1.31922255, -0.21850678, -1.4762087 ,  0.16366093, -1.59021671,\n",
       "        1.8889629 ,  0.45722732,  1.26641864, -0.6860761 , -1.40076015,\n",
       "       -1.8921931 ,  0.81244136,  0.07828237, -0.84674955, -0.81925387,\n",
       "       -0.55328578,  0.81129115,  0.4474974 ,  1.26615717, -1.60156676,\n",
       "       -0.4521614 , -0.08223389,  1.49079291, -1.83017016,  1.36815201,\n",
       "       -0.52165939, -1.20970226, -0.69007858, -0.29817487,  1.24836772,\n",
       "       -1.58428388, -1.43865683,  0.80068886,  0.53985917,  0.71161912,\n",
       "       -1.7482289 , -0.31718016, -0.64399283, -0.52932084,  1.63784112,\n",
       "       -0.47074024, -0.86771301,  0.39719802, -0.49202237,  1.37837589,\n",
       "        0.16773971, -0.34769822, -1.01134552, -0.31292874,  1.23371329,\n",
       "       -1.09138699, -1.2818121 , -1.13066292, -0.48744051,  1.8889629 ,\n",
       "        1.29457222, -1.407566  ,  0.9568031 , -0.65089396,  0.56097292,\n",
       "        1.84551299,  0.68740903, -0.82233086,  1.02959852,  0.3258791 ,\n",
       "        0.44119434,  0.12395949, -1.4999421 ,  0.78405952,  1.62906333,\n",
       "       -0.52431797,  1.50707391,  0.10140215, -0.98436365,  0.50443942,\n",
       "       -0.0659943 , -0.11863172, -0.12224166,  0.28792797, -1.4311736 ,\n",
       "        0.44977973, -0.72036265, -0.52752805,  1.29979291, -1.69261732,\n",
       "       -0.26837508, -0.62896866, -0.84349336, -1.23026738, -0.65836386,\n",
       "        0.40402826, -0.99402267, -1.58571652, -1.17815761, -1.34831822,\n",
       "        1.8889629 , -0.77870804, -0.5740352 , -0.20652094, -0.74913898,\n",
       "       -1.39790408,  0.55487236, -0.35911524, -0.84670812,  1.50049204,\n",
       "        0.97197326, -0.85560033,  0.74767098,  0.21468055, -0.51080206,\n",
       "        0.22856688, -0.41121274,  0.87759784,  0.92651897, -0.72204038,\n",
       "       -0.5813031 , -0.83151486,  0.76706538,  1.37721215,  0.5019947 ,\n",
       "        0.4934004 ,  0.43427845, -1.30902452, -1.31567136,  0.93812832,\n",
       "       -0.79721121, -1.43472811, -0.44017072,  0.88795263, -0.47427118,\n",
       "       -0.25965767, -1.61695313,  0.84922857, -0.28617343, -0.29826125,\n",
       "        0.03550109,  0.25986814,  0.35741315, -1.02912892, -1.20335537,\n",
       "       -0.26514728,  0.49169555, -1.01124499,  1.64001229,  0.31864001,\n",
       "        1.495894  ,  1.8889629 ,  1.10766714, -0.77659366, -0.55570548,\n",
       "       -0.66255595,  0.54835715,  1.68871358,  1.41644515, -1.66112454,\n",
       "       -1.46377101,  0.58344812,  0.39401956,  0.42270417,  0.97993394,\n",
       "       -0.90008984,  0.95381174,  0.43602137,  1.71462634, -0.73780303,\n",
       "       -0.62903417,  1.49003026, -1.85880074,  1.74951492, -1.55882586,\n",
       "        0.69378789, -0.4364896 , -1.1820291 , -0.22972231, -1.04063667,\n",
       "        1.35842235, -0.98372034, -1.15447467,  0.77002812, -0.94109791,\n",
       "       -1.52889736, -1.8921931 , -1.43367881, -0.28167863,  0.21747227,\n",
       "        0.86646001,  0.04604269,  1.78835153, -0.98020747, -1.2467013 ,\n",
       "        0.6799534 ,  1.8889629 ,  1.38950515, -0.10292054,  0.12597961,\n",
       "        0.20997403,  0.98000353, -0.78529188, -0.69287712, -0.19370714,\n",
       "        0.4821441 , -0.22325236, -0.56366154,  0.11952411,  0.76556743,\n",
       "        1.71487685, -0.59231941, -0.41073072,  1.8889629 ,  0.59715206,\n",
       "        0.08879312,  0.46798248, -1.62339372, -0.53509592,  1.8889629 ,\n",
       "       -0.39204633,  0.73337379,  1.4406314 ,  1.40558588,  0.24455862,\n",
       "       -0.51130503, -0.15319082, -0.50794284,  1.22939988,  1.6157547 ,\n",
       "        0.35356286, -0.09931681, -0.98392078,  1.13741625, -0.02487874,\n",
       "       -1.38840341, -0.83435397,  1.20243824, -0.56400601, -0.14134453,\n",
       "       -0.15358303,  0.36596281,  1.12332897,  1.2066957 , -0.69318898,\n",
       "        0.59404533,  0.89174066,  1.2056821 ,  0.74711341,  1.39910764,\n",
       "        1.46264228, -0.4142402 , -0.50713356, -0.31322238,  0.83327726,\n",
       "        1.82637637,  1.58411258,  0.36383577, -0.221879  , -1.31868821,\n",
       "        1.26523621,  0.0996475 ,  0.39320364, -1.28413789, -0.99017338,\n",
       "       -0.00706627, -0.01140761,  0.4962114 ,  1.28865896, -1.39942581,\n",
       "        0.62589216,  0.18401034, -1.35098527,  1.79337989,  1.8889629 ,\n",
       "        0.31644781, -0.53778219,  0.47992747,  0.01768773, -1.73497674,\n",
       "       -0.38298024, -0.94679751,  0.91841007, -1.8921931 , -0.58462545,\n",
       "        1.63934756, -0.3618056 ,  0.15062588, -0.91847023,  0.22843444,\n",
       "       -0.62090943,  0.44287869, -1.86276954,  1.21399605,  0.06680125,\n",
       "       -1.00979027, -1.52190376,  0.29922143,  0.10990892, -0.37041975,\n",
       "       -0.03920628,  1.08959527,  1.16052888, -0.08112818, -0.70463173,\n",
       "       -0.54490188, -0.67860662, -0.14781814,  0.28694516, -1.11439858,\n",
       "       -0.91139636, -0.5437501 ,  0.5158589 , -1.43492027, -0.58325568,\n",
       "        0.99907947, -0.77876078, -1.50439566,  1.63861982, -1.8921931 ,\n",
       "       -0.80991075,  1.38872766,  1.07455786,  1.8889629 ,  0.97829759,\n",
       "        0.63566852, -1.03845889, -0.2652931 ,  0.0684155 , -0.55679125,\n",
       "        0.72446839,  1.18216304,  0.27602716,  0.19686939,  0.54361496,\n",
       "        0.78549303,  1.6369347 , -0.7817144 , -1.49410422,  0.88169943,\n",
       "       -0.42842499, -1.05787872, -0.48821404,  0.5697552 ,  0.46026394,\n",
       "       -0.28816297,  0.43762705,  0.66800483,  0.95776286,  0.93564176,\n",
       "       -0.65066424,  0.73837022,  0.75017584,  1.72245966, -1.00651199,\n",
       "       -0.436451  , -0.1961299 ,  0.46249908, -0.33992792,  0.7648356 ,\n",
       "       -1.32424875, -0.27054838,  0.53263193, -1.14802633, -0.97475186,\n",
       "        1.24401295,  0.13592401,  0.59374147,  0.36389875, -1.7017106 ,\n",
       "        0.58286491,  0.57781308,  0.51612208, -0.75178538, -1.8921931 ,\n",
       "        1.24803245,  0.23179574, -1.23770916,  0.54375578, -1.18862261,\n",
       "        0.30624734, -0.7681473 , -0.84412768,  0.48016613, -1.12167379,\n",
       "        0.76813451,  0.0659606 , -1.50130096, -0.01736149,  0.84822094,\n",
       "       -1.17674434,  0.2125573 ,  0.21183601,  1.12715974,  0.89402189,\n",
       "       -0.97300808, -0.21283   ,  0.92050923,  0.3732915 , -0.76523885,\n",
       "        1.08458017,  0.14402748, -0.40520488,  0.47096018, -0.84143457,\n",
       "       -0.77306307, -0.83763234,  0.2112403 ,  0.56425048, -0.24504956,\n",
       "       -0.16098135,  1.41348934, -0.63934405,  0.3178919 ,  0.35694448,\n",
       "        0.33707741, -0.9474922 ,  0.95635223, -1.11236873,  0.2680272 ,\n",
       "       -1.78853586,  1.20905245, -0.96810419,  0.4316065 , -0.33675607,\n",
       "       -1.8921931 , -0.63240629,  0.03127796, -1.19996157, -1.8921931 ,\n",
       "       -0.01266547, -0.35321867, -0.26383058,  0.65934884, -0.43674082,\n",
       "       -0.24639091,  0.2400062 , -1.24188538, -0.52038394, -0.37116458,\n",
       "       -1.20600132, -0.88926625,  0.53574317,  1.12790105, -0.49022208,\n",
       "        0.00575165,  1.04697489, -0.10478898, -1.8921931 , -1.24832872,\n",
       "        1.13101202,  0.25669166,  0.76917009, -0.6841298 , -0.32732743,\n",
       "       -0.50046008, -0.22720136,  0.92149661,  1.17109186,  0.13580811,\n",
       "       -1.38082037,  0.74471108,  1.26485173,  0.23379778, -0.02529372,\n",
       "        0.9660019 , -0.13906167,  0.4823705 ,  1.21251615, -1.52098684,\n",
       "        1.8889629 ,  0.11461166, -0.85081304,  1.81770061,  0.98540802,\n",
       "        1.38166678,  0.50648652, -1.26349605, -0.69199255, -0.54411386,\n",
       "        0.92848486,  1.04657603, -0.76527778,  1.40225726,  0.92488863,\n",
       "       -1.55540446,  0.88479834, -0.7457946 ,  0.38308287, -0.23257185,\n",
       "       -0.9326978 ,  0.68359603,  0.58883792,  1.48492708,  1.28525208,\n",
       "        0.73007148, -0.23246042, -0.33190652,  0.29296779, -1.45649617,\n",
       "        1.19739944,  0.78203457, -0.4140877 , -0.09642635,  0.26785638,\n",
       "        1.04388439, -0.05450479,  0.48334608,  0.27485699, -1.8921931 ,\n",
       "        0.03441919,  1.8889629 ,  0.39318981,  1.8889629 ,  0.84338464,\n",
       "        1.19386952, -1.53690045, -0.22046934,  0.74451462,  1.48561284,\n",
       "        0.71387033, -1.3896996 , -0.92952396, -0.69320809, -0.31288464,\n",
       "       -1.1603133 ,  0.87442885,  0.44398112,  0.72429334,  1.03181318,\n",
       "       -0.78140041, -1.42229331, -1.55029521,  0.18524284,  0.38709586,\n",
       "        0.34424271, -0.56104598,  1.11389861, -1.79752545,  1.8889629 ,\n",
       "        0.65631367, -1.63878759, -0.49526203, -0.04111328,  0.06385043,\n",
       "       -1.8921931 , -1.8921931 ,  0.38980137, -1.11241367,  0.53055741,\n",
       "        0.08941119,  1.30778772,  0.51676116, -0.72349828,  0.51540401,\n",
       "        0.21204343,  0.85644156,  1.1873621 ,  1.34793792, -1.63177382,\n",
       "        0.81104995, -1.8921931 ,  0.58512307,  0.15163165,  1.49541262,\n",
       "       -1.14639696,  1.05634967,  0.02465336,  1.48250079, -0.0315329 ,\n",
       "        1.11140275,  0.44323563, -0.05190164, -1.36578605,  0.22470214,\n",
       "       -1.61133776,  1.12007804,  1.6913929 , -1.33085466, -0.2648919 ,\n",
       "       -0.56679643, -0.99512426,  1.01766387, -0.33187049,  1.38411725,\n",
       "       -1.3437539 , -0.78272685,  1.13422042,  0.70825056,  1.41342529,\n",
       "        1.58925915,  1.19372381, -1.03345923, -0.76825727,  0.60901888,\n",
       "       -1.07389913, -0.63778309, -1.39404935, -0.96246051, -1.13245952,\n",
       "       -0.73168625, -0.79768424, -0.37665248,  0.28912755,  0.24804247,\n",
       "       -0.94095017,  0.59944258,  0.99770895,  0.24944153,  1.24001124,\n",
       "        0.38358936, -0.10366066, -0.19026352, -0.8772803 , -0.95640376,\n",
       "       -1.42441993,  0.60514395, -0.61435991,  1.8889629 ,  1.41058966,\n",
       "       -0.63043759,  0.74483791,  0.97276528, -1.072671  , -0.72404123,\n",
       "        0.23389516, -1.18944494,  0.78974432, -0.76219653,  0.91544925,\n",
       "       -0.33707908, -0.3857723 ,  1.22039234,  0.1715241 , -1.20129522,\n",
       "       -1.30428024,  1.33796154,  0.80842977,  0.98878132,  0.17636577,\n",
       "        0.28027414, -0.10138026,  1.2540591 , -0.67075675,  0.08400463,\n",
       "       -0.4199266 ,  1.8889629 , -0.23840288,  1.09434821, -0.14246301,\n",
       "        0.77188945,  0.6424347 ,  1.82095396, -0.10079451, -1.42908485,\n",
       "        1.8889629 ,  0.99428218, -0.04054729,  0.99870557, -1.34608868,\n",
       "       -0.72810349,  0.48648111,  0.53966358,  0.02413985, -0.63648794,\n",
       "       -0.82110562, -1.0892313 , -1.21635383, -0.61815614, -0.28995503,\n",
       "        1.8889629 ,  1.76909596,  1.8889629 , -0.91548757,  1.76238297,\n",
       "        0.90941465,  0.42936321,  0.94545381, -0.25575522, -1.32395511,\n",
       "       -0.74434736, -0.1081052 ,  0.09315959, -1.8921931 ,  1.67028412,\n",
       "        0.65399931, -0.69810983, -0.40741741,  1.21854328, -1.23364388,\n",
       "        0.5008398 ,  1.25326782,  1.8889629 , -0.18024489,  0.36305152,\n",
       "        0.22819481,  1.31802971,  0.67331437,  0.80686972,  0.33039454,\n",
       "       -0.46436488,  0.87163064,  1.09650028, -0.88627645, -1.48689626,\n",
       "        1.84380835,  1.40712909,  1.84368684, -0.53272194, -1.45559561,\n",
       "        1.88314188,  0.10178573, -0.99749512, -1.25470718, -1.05593025,\n",
       "       -0.78372094, -1.8921931 , -1.27770131,  1.35347812,  0.20558781,\n",
       "       -1.29883249, -1.64977503,  0.86020451,  1.8889629 , -0.22887986,\n",
       "       -0.27724267,  1.04503121, -0.51126315,  1.15780991, -0.97804696,\n",
       "        1.343992  ,  1.41110133, -0.86537885,  1.14141101, -0.71601536,\n",
       "        0.78725274,  0.64159143,  0.28112211, -1.8921931 ,  0.9029919 ,\n",
       "       -0.51430893,  0.97658728,  0.60730765,  0.09015472, -0.6803365 ,\n",
       "       -1.57321833, -0.03592134,  0.06392659, -1.8921931 ,  1.67225935,\n",
       "       -1.37183324,  1.61449799, -1.82591721, -0.04111034, -0.07316426,\n",
       "       -1.8921931 , -0.68215953, -0.61476464,  1.24588432, -1.15023613,\n",
       "       -0.1827094 ,  0.92506869,  0.45669624, -0.49554055, -0.11940134,\n",
       "        1.61958871,  0.15707364,  1.6136954 , -0.92925411,  0.83010715,\n",
       "       -0.0639367 ,  1.8889629 , -0.25908489,  1.09780313, -1.8921931 ,\n",
       "       -1.8921931 , -0.13696353, -0.3640032 , -0.20654946, -0.78591136,\n",
       "       -1.07990976, -0.71993109,  0.55437425, -0.18710842,  0.3406275 ,\n",
       "       -1.8921931 ,  0.04267645,  1.61819486,  0.13484729,  1.00466267,\n",
       "        0.94117124, -1.83482116,  1.65182806, -0.31837294, -1.15661247,\n",
       "        0.65466293, -0.45728042, -1.73731395, -0.15463843,  1.56045306,\n",
       "       -0.95614649,  1.8889629 , -0.15604196,  0.41199861, -0.72817874,\n",
       "       -1.51069804,  1.23877535, -0.99930426, -0.52236413, -0.51412489,\n",
       "        1.04551139,  0.2537022 , -1.8921931 , -1.13872124, -0.03354701,\n",
       "       -1.2922643 , -0.73241473,  0.3507743 ,  0.95363715, -0.20733897,\n",
       "       -1.8921931 , -1.69276506, -0.7373566 , -0.78384913,  0.65320579,\n",
       "       -0.73517496,  0.21593883, -1.17354152, -0.00289814, -1.38385235])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_1[0][0]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the electrode number:30\n",
      "<class 'list'>\n",
      "[30]\n",
      "Enter the number of stimuli:160\n",
      "Please define what should be predicted (1 for EEG or 2 for stimulus):1\n",
      "please specify the number of layers and neurons per desired layers: 1\n",
      "[1]\n",
      "<class 'list'>\n",
      "enter the lambda / learning rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "# parametres\n",
    "\n",
    "eltmp = input ('''Enter the electrode number:''')\n",
    "electi = list(map(int, eltmp.split()))    #separation of the different responses and recovery in the form of a list of integers\n",
    "print (type(electi))\n",
    "print(electi)\n",
    "\n",
    "stim = input ('''Enter the number of stimuli:''')\n",
    "stim = int(stim)\n",
    "\n",
    "relation = input('''Please define what should be predicted (1 for EEG or 2 for stimulus):''')\n",
    "\n",
    "if relation == '1':\n",
    "    source_Y = electi[0]    #retrieving the electrode number as a whole number - implies that there is only one electrode chosen in this direction\n",
    "    source_X = [0]          #conversion of the stimuli line in the form of a list - necessary for the for loop: see below - extraction X\n",
    "\n",
    "elif relation == '2':\n",
    "    format_1 = np.flip(format_1,2)     # data inversion according to the time dimension - problem ????\n",
    "    source_Y = 0\n",
    "    source_X = electi\n",
    "\n",
    "hiLaSi_tmp = input('''please specify the number of layers and neurons per desired layers: ''')\n",
    "hidden_layers_size = list(map(int, hiLaSi_tmp.split()))\n",
    "print (hidden_layers_size)\n",
    "print (type(hidden_layers_size))\n",
    "\n",
    "learning_rate = input('''enter the lambda / learning rate: ''')\n",
    "learning_rate = float(learning_rate)      # conversion as a decimal number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### separation of train tests / valid / test\n",
    "\n",
    "train_num = int(np.around(len(trials) * 0.8))\n",
    "valid_num = int(np.around(len(trials) * 0.1))\n",
    "test_num = len(trials) - train_num - valid_num\n",
    "\n",
    "trials_train = trials[0:train_num]\n",
    "trials_valid = trials[train_num:train_num+valid_num]\n",
    "trials_test = trials[train_num+valid_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape =  (129360, 1)\n",
      "y_valid.shape =  (15960, 1)\n",
      "y_test.shape =  (15960, 1)\n"
     ]
    }
   ],
   "source": [
    "# extract data from format_1\n",
    "\n",
    "def extract_Y (batch_trials, batch_num):              #creation of a function to recover y - simplification of reading\n",
    "    \n",
    "    y_tmp=format_1[source_Y, batch_trials, stim:]     #recovery of Y in the form of a matrix of 154 * 840\n",
    "    y_tmp=np.reshape(y_tmp, ((time_points-stim)*batch_num))    #passage through the list of 129 360 values ​​(test 0, test 1, ... test 153)\n",
    "    y_tmp=np.matrix(y_tmp)                           #1 * 129360 matrix conversion\n",
    "    y_tmp=np.transpose(y_tmp)                        #transposition into a matrix of 129360 * 1, matrix equal to that of Matlab (necessary for the rest)\n",
    "    return y_tmp                                     #returns the content of y_tmp\n",
    "    \n",
    "y_train = extract_Y (trials_train, train_num)\n",
    "print (\"y_train.shape = \", y_train.shape)\n",
    "y_valid = extract_Y (trials_valid, valid_num)\n",
    "print (\"y_valid.shape = \", y_valid.shape)\n",
    "y_test = extract_Y (trials_test, test_num)\n",
    "print (\"y_test.shape = \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (129360, 160)\n",
      "x_valid.shape =  (15960, 160)\n",
      "x_test.shape =  (15960, 160)\n"
     ]
    }
   ],
   "source": [
    "def extract_X (batch_trials, batch_num):                     #creation of a function to recover x - simplification of reading\n",
    "    x_tmp = [[]]*((time_points-stim)*batch_num)                       #creation of an empty list x_tmp of size (129630,)\n",
    "    x_tmp = np.matrix(x_tmp)                                #conversion as a matrix 129630 * 0 (number of lines good)\n",
    "    x_tmp = np.transpose(x_tmp)                             #transposition 1: form 0 * 129630, necessary at n.append\n",
    "                                                            #avoid the transposition line in the for loop (2 transpo instead of 160\n",
    "    for i in source_X:                                      #reading the source list -> reading each electrode number if flip\n",
    "        k = 0\n",
    "        while k < stim:                                     #160 loops - recovery of the 840 values ​​of each test (in the form test 1, test 2, ...), shifted by 1 at each iteration\n",
    "            tmp = format_1[i, batch_trials, k:(time_points-stim+k)]    #see extract_Y\n",
    "            tmp = np.reshape(tmp,((time_points-stim)*batch_num))\n",
    "            tmp = np.matrix(tmp)\n",
    "            \n",
    "            x_tmp = np.append(x_tmp, tmp, axis=0)           #concatenation of the matrix tmp 1 * 129360 at the end of the matrix x_tmp (k + 1) * 129360\n",
    "            k = k+1\n",
    "    x_tmp = np.transpose(x_tmp)                             #transposition 2: form 129360 * (160 * nbr_électrode)\n",
    "    return x_tmp\n",
    "        \n",
    "x_train = extract_X (trials_train, train_num)\n",
    "print (\"x_train.shape = \", x_train.shape)\n",
    "x_valid = extract_X (trials_valid, valid_num)\n",
    "print (\"x_valid.shape = \", x_valid.shape)\n",
    "x_test = extract_X (trials_test, test_num)\n",
    "print (\"x_test.shape = \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow - Linear regression cf github\n",
    "\n",
    "training_epochs = 100\n",
    "display_step = 10\n",
    "\n",
    "train_X = x_train\n",
    "train_Y = y_train\n",
    "n_samples_train = train_X.shape[0]\n",
    "\n",
    "valid_X = x_valid\n",
    "valid_Y = y_valid\n",
    "n_samples_valid = valid_X.shape[0]\n",
    "\n",
    "test_X = x_test\n",
    "test_Y = y_test\n",
    "n_samples_test = test_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160, 1]\n",
      "WARNING:tensorflow:From /home/vasu/PycharmProjects/EEG/test/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "    # tf Graph Input\n",
    "(size_L, size_C) = train_X.shape\n",
    "X = tf.placeholder(\"float\", [None,size_C])\n",
    "Y = tf.placeholder(\"float\",[None,1])\n",
    "\n",
    "W={}   #creation of a dictionary W, each key of the dictionary will contain the weights of a layer\n",
    "bb={}    #same for baby\n",
    "\n",
    "    # Set model weights\n",
    "hidden_layers_size = [size_C]+hidden_layers_size    #we add the number of columns of the base matrix (ex: 160) to the list of layers\n",
    "print(hidden_layers_size)                           # ex : 160 2 3 1\n",
    "\n",
    "for i,j in enumerate(hidden_layers_size[:-1]):\n",
    "        W_tmp = tf.Variable(tf.fill([j,hidden_layers_size[i+1]],rng.randn()))  # For each index i having the value j excluding the last layer (ex: 0: 160, 1: 2, 2: 3)\n",
    "        W[i+1] = W_tmp   #we create a tensorflow variable W having for size (layer * next_layer) #ex: 3 layers W: 160 * 2, 2 * 3, 3 * 1\n",
    "    \n",
    "        bb_tmp = tf.Variable(tf.fill([hidden_layers_size[i+1]],rng.randn()))   #we create a tensorflow variable W having for size (next_layer)\n",
    "        bb[i+1] = bb_tmp    #we add a new key to the dictionary, containing the new layer (key index: 1 2 3 ... to change according to preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 160)\n",
      "{1: <tf.Variable 'Variable:0' shape=(160, 1) dtype=float32_ref>}\n",
      "{1: <tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref>}\n"
     ]
    }
   ],
   "source": [
    "print (X.shape)\n",
    "print (W)\n",
    "print (bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: <tf.Tensor 'xw_plus_b:0' shape=(?, 1) dtype=float32>}\n",
      "Tensor(\"xw_plus_b:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    " # Construct a linear model\n",
    "#hiddenX = tf.nn.xw_plus_b(X,W1,bb1)\n",
    "#pred = tf.nn.xw_plus_b(hiddenX,W2,bb2)\n",
    "\n",
    "hidden_X_tmp = X  #corresponds to the data that we enter (extract_X)\n",
    "hidden_X = {}      #similar to W: creation of a dictionary storing the hidden_X of the different layers\n",
    "i = 1\n",
    "\n",
    "while i <= len(W):\n",
    "    hidden_X_tmp = tf.nn.xw_plus_b(hidden_X_tmp,W[i],bb[i])   #2 times the same values ​​for two neurons of the same layer ... the problem may come from here\n",
    "    hidden_X[i] = hidden_X_tmp\n",
    "    i=i+1\n",
    "\n",
    "print (hidden_X)\n",
    "pred = hidden_X[len(W)]     #pred corresponds to the hidden_X of the last layer - review the indexing?\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Mean squared error\n",
    "cost_train = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples_train)\n",
    "cost_valid = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples_valid)\n",
    "cost_test = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples_test)\n",
    "    # Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "   # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        sess.run(optimizer, feed_dict={X: train_X, Y: train_Y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            cost = sess.run(cost_train, feed_dict={X: train_X, Y:train_Y})\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(cost))\n",
    "            costV = sess.run(cost_valid, feed_dict={X: valid_X, Y: valid_Y})\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"valid_cost=\", \"{:.9f}\".format(costV))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost_train, feed_dict={X: train_X, Y: train_Y})\n",
    "    print (\"training_cost = \", training_cost) \n",
    "    test_cost = sess.run(cost_test, feed_dict={X: test_X, Y: test_Y})\n",
    "    print (\"test_cost = \", test_cost)\n",
    "    \n",
    "       #Graphic display\n",
    "        \n",
    "    T = np.arange(0, 0.00625*stim, 0.00625)                       # creation of the time variable (on 1s) for the abscissa\n",
    "    \n",
    "    nbr_elct = len(electi)\n",
    "    (Lo_W, la_W) = W[1].shape\n",
    "    Lo_W = int(Lo_W)                                              # number of lines: inputs (160 * nbr_electrodes, for example)\n",
    "    la_W = int(la_W)                                              # number of columns: number of neurons in the layer\n",
    "    \n",
    "    z=0\n",
    "    while z < nbr_elct:                                           # for each electrode\n",
    "        z_1=0 \n",
    "        while z_1 < la_W:                                         # for each neuron of the W layer [1]\n",
    "            W_tmp = tf.slice(W[1], [0, z_1], [Lo_W, 1])           # slice: starting value [line 0, column of the neuron], dimensions of the section [160 * nbr_electrodes lines, 1 column])\n",
    "            W_tmp = tf.slice(W_tmp, [z*stim, 0], [stim, 1])       # slice: starting value [first value of the new electrode, column 0], dimensions of the section [160 lines, 1 column]\n",
    "            print (W_tmp.shape)\n",
    "            \n",
    "            plt.plot(T, sess.run(W_tmp), label= (\"neurone_\", z_1, \"layer_1, electrode_\", electi[z]))\n",
    "            z_1 = z_1+1\n",
    "            \n",
    "        z = z+1\n",
    "        \n",
    "    plt.legend()    \n",
    "    plt.show()\n",
    "    #plt.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "print(len(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
